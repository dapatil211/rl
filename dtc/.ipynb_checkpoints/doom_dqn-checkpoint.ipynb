{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools as it\n",
    "import pickle\n",
    "from time import time, sleep\n",
    "\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from skimage import transform, io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning settings\n",
    "learning_rate = 0.00025\n",
    "# learning_rate = 0.0001\n",
    "discount_factor = 0.99\n",
    "epochs = 20\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "# NN learning settings\n",
    "batch_size = 64\n",
    "\n",
    "# Training regime\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "# Other parameters\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "model_savefile = \"/tmp/weights.dump\"\n",
    "# Configuration file path\n",
    "config_file_path = '../ViZDoom/scenarios/defend_the_center.cfg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vizdoom(config_file_path):\n",
    "    print(\"Initializing doom...\")\n",
    "    game = DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(False)\n",
    "    game.set_mode(Mode.PLAYER)\n",
    "    game.set_screen_format(ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n"
     ]
    }
   ],
   "source": [
    "game = initialize_vizdoom(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    image = transform.resize(image, resolution)\n",
    "    image = np.ascontiguousarray(image, dtype=np.float32)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "def get_current_state():\n",
    "    return preprocess(game.get_state().screen_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [list(a) for a in it.product([0, 1], repeat=game.get_available_buttons_size())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay:\n",
    "    curIndex = 0\n",
    "    size = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.replay = []\n",
    "    def add(self, s1, s2, action, reward):\n",
    "        if self.size == self.capacity:\n",
    "            self.replay[self.curIndex] = (s1, s2, action, reward)\n",
    "        else:\n",
    "            self.replay.append((s1, s2, action, reward))\n",
    "            self.size = min(self.size + 1, self.capacity)\n",
    "        self.curIndex = (self.curIndex + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.replay, batch_size)\n",
    "replay = Replay(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=6, stride=3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, stride=2)\n",
    "        self.fc1 = nn.Linear(192, 128)\n",
    "        self.fc2 = nn.Linear(128, len(actions))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 192)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.SGD(dqn.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_q(input):\n",
    "    input = Variable(input)\n",
    "    return dqn(input)\n",
    "\n",
    "def get_action(input):\n",
    "    qs = get_q(input)\n",
    "    val, ind = torch.max(qs, 1)\n",
    "    return ind.data.numpy[0]\n",
    "\n",
    "def learn_replay():\n",
    "    if batch_size <= replay.size:\n",
    "        sample = zip(*replay.sample(batch_size))\n",
    "        s1, s2, action, reward = sample\n",
    "        \n",
    "        s1 = torch.stack(s1)\n",
    "        s1 = s1.view(s1.size()[0], 1, s1.size()[1], s1.size()[2])\n",
    "        \n",
    "        action = np.array(action)\n",
    "        q1 = get_q(s1)\n",
    "        q1 = q1.data.numpy()[np.arange(action.size), action]\n",
    "        print(s2)\n",
    "        s2 = torch.stack(s2)\n",
    "        s2 = s2.view(s2.size()[0], 1, s2.size()[1], s2.size()[2])\n",
    "        q2 = get_q(s2).data.numpy()\n",
    "        q2 = np.max(q2, 1)\n",
    "        #q_func = np.vectorize(lambda s:np.max(get_q(s.view(1, 1, resolution[0], resolution[1]))) if s else 0)\n",
    "        #q2 = q_func(s2)\n",
    "        \n",
    "        y = reward + discount_factor * q2\n",
    "        \n",
    "        q1 = Variable(torch.from_numpy(q1), re)\n",
    "        y = Variable(torch.from_numpy(y).float())\n",
    "        loss = mse(q1, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(eps):\n",
    "    action = 0\n",
    "    s1 = get_current_state()\n",
    "    if random.random() < eps:\n",
    "        action = random.randint(0, len(actions)-1)\n",
    "    else:\n",
    "        action = get_best_action(cur_state)\n",
    "    reward = game.make_action(actions[action])\n",
    "    s2 = None if game.is_episode_finished else get_current_state()\n",
    "    replay.add(s1, s2, action, reward)\n",
    "    learn_replay()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 EPOCH\n",
      "(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8708aa517966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnum_episodes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlearning_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a9a745d222d6>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(eps)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mreplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlearn_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-bc5f4f761762>\u001b[0m in \u001b[0;36mlearn_replay\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "EPS_START = 1.\n",
    "EPS_END = .1\n",
    "EPS_CONST = NUM_EPOCHS * .1\n",
    "EPS_DECAY = NUM_EPOCHS * .70\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print \"%d EPOCH\" % i\n",
    "    learning_steps = 0\n",
    "    num_episodes = 0\n",
    "    epsilon = 0\n",
    "    if i <= EPS_CONST:\n",
    "        epsilon = EPS_START\n",
    "    elif i <= EPS_DECAY:\n",
    "        epsilon = EPS_START - (i - EPS_CONST) / (EPS_DECAY - EPS_CONST) * (EPS_START - EPS_END)\n",
    "    else:\n",
    "        epsilon = EPS_END\n",
    "    scores = []\n",
    "    while learning_steps < learning_steps_per_epoch:\n",
    "        if(game.is_episode_finished() or num_episodes == 0):\n",
    "            scores.append(game.get_total_reward())\n",
    "            game.new_episode()\n",
    "            num_episodes += 1\n",
    "        learn(epsilon)\n",
    "        learning_steps += 1\n",
    "    print(scores.mean())\n",
    "    torch.save(dqn, model_savefile)\n",
    "\n",
    "game.close()\n",
    "print(\"======================================\")\n",
    "print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "# Reinitialize the game with window visible\n",
    "game.set_window_visible(True)\n",
    "game.set_mode(Mode.ASYNC_PLAYER)\n",
    "game.init()\n",
    "\n",
    "for _ in range(episodes_to_watch):\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        state = get_current_state()\n",
    "        state = state.view(1, 1, resolution[0], resolution[1])\n",
    "        best_action_index = get_action(state)\n",
    "\n",
    "        # Instead of make_action(a, frame_repeat) in order to make the animation smooth\n",
    "        game.make_action(actions[best_action_index])\n",
    "        #for _ in range(frame_repeat):\n",
    "        #    game.advance_action()\n",
    "\n",
    "    # Sleep between episodes\n",
    "    sleep(1.0)\n",
    "    score = game.get_total_reward()\n",
    "    print(\"Total score: \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
